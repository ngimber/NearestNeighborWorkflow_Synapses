{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of a random protein distribution in a spherical synapse\n",
    "#### (SynCAM was distributed to the surface, while MPP2 and PSD95 were distributed in the postsynaptic lumen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import os\n",
    "import ntpath\n",
    "from tqdm import trange\n",
    "import scipy.spatial\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Simulation Function\n",
    "#### This function takes a list of subject and reference diameters and places each subjects with a defined number of randomly chosen references in a 3d sphere with defined diameter and calculates the NN distribution for all (or, for the sake of speed a limeted number of) subjects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to transfer Volumes into diameters and back\n",
    "\n",
    "def VolToDiameter(volume):\n",
    "    d=2*(volume*((3/4)/math.pi))**(1/3)\n",
    "    return d\n",
    "\n",
    "def DiameterToVolume(d):\n",
    "    volume=((d/2)**(3))/(((3/4)/math.pi))\n",
    "    return volume\n",
    "#######################################################\n",
    "\n",
    "\n",
    "#volsA and volsB:  volumes, diameterRegion: diameter of single regions e.g. cells or synapses, refsPerRegion: number of references per analyzed region, maxSubjects: number of maximal subjects per simulation use max to analyze all or add number, SubjOnSurface,RefOnSurface, SubjOnSurface/RefOnSurface: restricts the one or the other channel onto the surface, if Ture. \n",
    "def NN(volsSubj,volsRef,diameterRegion,refsPerRegion,maxSubjects,SubjOnSurface,RefOnSurface):\n",
    "    \n",
    "    centerDistances=[]\n",
    "    surfaceDistances=[]\n",
    "    \n",
    "    volsSubj=volsSubj[volsSubj<DiameterToVolume(diameterRegion)*0.8]#reject objects that would clot the sphere\n",
    "    volsRef=volsRef[volsRef<DiameterToVolume(diameterRegion)*0.8]#reject objects that would clot the sphere\n",
    "        \n",
    "\n",
    "    if maxSubjects==max:\n",
    "        tmp=len(volsSubj)\n",
    "    else:\n",
    "        if maxSubjects<len(volsSubj):\n",
    "            tmp=maxSubjects\n",
    "        else:\n",
    "            tmp=len(volsSubj)\n",
    "    \n",
    "    for i in range (tmp):\n",
    "        \n",
    "        #pick random volumes\n",
    "\n",
    "        randomSubj=random.sample(list(volsSubj),1) #pick one subject\n",
    "        randomRef=random.sample(list(volsRef),int(refsPerRegion)) #pick multiple references\n",
    "        \n",
    "        \n",
    "        tmpCenter=[]\n",
    "        tmpSurface=[]\n",
    "                \n",
    "        \n",
    "        for j in range(int(refsPerRegion)):\n",
    "        \n",
    "        #place objects into the defined region, 0 is the center of the sphere  \n",
    "            \n",
    "\n",
    "            diameterSubj=VolToDiameter(randomSubj[0])\n",
    "            diameterRef=VolToDiameter(randomRef[j])            \n",
    "\n",
    "            radBorderSubj=(diameterRegion/2)-(diameterSubj/2) #define radius in which Subjects can be placed\n",
    "            radBorderRef=(diameterRegion/2)-(diameterRef/2) #define radius in which Reference can be placed\n",
    "\n",
    "            \n",
    "            #random subject and reference center within region --> strategy: generate cube and reject ones with too high radius\n",
    "            radi=99999999999999\n",
    "            while (radi>radBorderSubj):\n",
    "                cSubj=[-radBorderSubj+random.random()*(2*radBorderSubj),-radBorderSubj+random.random()*(2*radBorderSubj),-radBorderSubj+random.random()*(2*radBorderSubj)]\n",
    "                radi=(np.sum(np.multiply(cSubj,cSubj)))**(1/2)#pytagoras from center\n",
    "            if(SubjOnSurface==True):\n",
    "                cSubj[2]=(((radBorderSubj**2)-(cSubj[0]**2)-(cSubj[1]**2))**0.5)*random.choice([-1, 1])\n",
    "                np.random.shuffle(cSubj)\n",
    "\n",
    "                    \n",
    "            radi=99999999999999\n",
    "            while (radi>radBorderRef):\n",
    "                cRef=[-radBorderRef+random.random()*(2*radBorderRef),-radBorderRef+random.random()*(2*radBorderRef),-radBorderRef+random.random()*(2*radBorderRef)]\n",
    "                radi=(np.sum(np.multiply(cRef,cRef)))**(1/2)#pytagoras from center\n",
    "            if(RefOnSurface==True):\n",
    "                cRef[2]=(((radBorderRef**2)-(cRef[0]**2)-(cRef[1]**2))**0.5)*random.choice([-1, 1])\n",
    "                np.random.shuffle(cRef)\n",
    "\n",
    "\n",
    "            \n",
    "            #calculate distance between centers and surface\n",
    "            deltaC=np.array(cSubj)-np.array(cRef)\n",
    "            distanceC=(np.sum(deltaC*deltaC))**(1/2)#pytagoras from delta array\n",
    "            distanceS=(distanceC-(diameterRef/2)-(diameterSubj/2))\n",
    "            distanceS=np.clip(distanceS,0,distanceS)\n",
    "                \n",
    "            tmpCenter+=[distanceC]\n",
    "            tmpSurface+=[distanceS]\n",
    "            \n",
    "\n",
    "        centerDistances+=[distanceC.min()]\n",
    "        surfaceDistances+=[distanceS.min()]\n",
    "                       \n",
    "    NNcenter=pd.DataFrame(np.array(centerDistances))\n",
    "    NNsurface=pd.DataFrame(np.array(surfaceDistances))\n",
    "    table=pd.concat([NNcenter, NNsurface], axis=1)\n",
    "    table=pd.DataFrame(table)\n",
    "    table.columns = ['NNcenter', 'NNsurface']\n",
    "    table\n",
    "    \n",
    "    return table    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the simulation on object from one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the list of object size\n",
    "PSD=pd.read_table(\"..../test_simulation/Ch2shifted-8pixels_Div21_hippoN_No23_647-PSD95_568-MPP2-488_SynCAM_405-vGlut_oil18_001_visit_2_SIR_ALX.tif_PSD95_Pipeline_V3.2.txt\", sep=\"\\t\")\n",
    "MPP2=pd.read_table(\"....//test_simulation/Ch2shifted-8pixels_Div21_hippoN_No23_647-PSD95_568-MPP2-488_SynCAM_405-vGlut_oil18_001_visit_2_SIR_ALX.tif_MPP2_Pipeline_V3.2.txt\", sep=\"\\t\")\n",
    "SynCAM=pd.read_table(\"....//test_simulation/Ch2shifted-8pixels_Div21_hippoN_No23_647-PSD95_568-MPP2-488_SynCAM_405-vGlut_oil18_001_visit_2_SIR_ALX.tif_SynCAM_Pipeline_V3.2.txt\", sep=\"\\t\")\n",
    "\n",
    "PSDvolsUM=PSD[\"Volume: Volume\"]*1000**6\n",
    "SynCAMvolsUM=SynCAM[\"Volume: Volume\"]*1000**6\n",
    "MPP2volsUM=MPP2[\"Volume: Volume\"]*1000**6\n",
    "\n",
    "\n",
    "#run the simulation\n",
    "significance=0.95\n",
    "rounds=10\n",
    "diameterRegion=0.8#µm (published postsynaptic size)\n",
    "volsSubj=SynCAMvolsUM\n",
    "volsRef=MPP2volsUM\n",
    "\n",
    "refsPerRegion= len(volsRef)/len(PSDvolsUM)\n",
    "tmp=NN(volsSubj,volsRef,diameterRegion,refsPerRegion,100,False,False)\n",
    "#volsA and volsB:  volumes, diameterRegion: diameter of single regions e.g. cells or synapses, refsPerRegion: number of references per analyzed region, maxSubjects: number of maximal subjects per simulation use max to analyze all or add number, SubjOnSurface,RefOnSurface, SubjOnSurface/RefOnSurface: restricts the one or the other channel onto the surface, if Ture.\n",
    "\n",
    "tmp.hist(bins=100,range=[-1,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining further Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Binning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "\n",
    "def binData(inData,firstcol,lastcol,minbin,maxbin,nbins):\n",
    "    myArray=inData.as_matrix()\n",
    "    \n",
    "    numrows = len(myArray)\n",
    "    numcols = len(myArray[0]) #[0] for cols. otherwise rows\n",
    "    if (lastcol==\"max\"):\n",
    "        lastcol=numcols\n",
    "\n",
    "    for j in range(firstcol, lastcol):\n",
    "       \n",
    "\n",
    "        data = myArray[:,j]\n",
    "        bin_means = binned_statistic(data, data, statistic=\"count\", bins=nbins, range=(minbin, maxbin))[0]\n",
    "        bin_edges= binned_statistic(data, data, statistic='count', bins=nbins, range=(minbin, maxbin))[1]\n",
    "        binnumber= binned_statistic(data, data, statistic='count', bins=nbins, range=(minbin, maxbin))[2]\n",
    "        bin_middle=bin_edges[0:len(bin_edges)-1]+((maxbin-minbin)/nbins/2)\n",
    "\n",
    "        bin_means = np.nan_to_num(bin_means)\n",
    "        bin_means=bin_means.tolist()       \n",
    "        \n",
    "        if (j==0):\n",
    "            allbins = np.vstack((bin_middle, bin_means))\n",
    "        else:\n",
    "            allbins = np.vstack((allbins, bin_means))\n",
    "        \n",
    "    allbins=allbins.T    \n",
    "    matrix=pd.DataFrame(allbins).set_index(0)\n",
    "    matrix.columns=inData.columns[firstcol:lastcol]     \n",
    "\n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(raw):\n",
    "    normalized=(raw/raw.max())-(raw.min()/raw.max())#notmalize 1-0\n",
    "    normalized=(raw/raw.mean())#normalize to sum\n",
    "    #normalized=raw\n",
    "    normalized=pd.DataFrame(normalized).fillna(0)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "#NormSubjDesi_RefERGIC=(SubjDesi_RefERGIC/SubjDesi_RefERGIC.max())-(SubjDesi_RefERGIC.min()/SubjDesi_RefERGIC.max())\n",
    "#pd.DataFrame(NormSubjDesi_RefERGIC).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Splitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(matrix,identifyer,axis):\n",
    "    splitted=matrix.filter(like=identifyer,axis=axis)\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\"\" BATCH PROCESSING \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate file lists per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"....\\_Annotation_And_Simulations\\noShift_and_simulations\"\n",
    "\n",
    "files=[]\n",
    "tmp=os.listdir(path)\n",
    "for each in tmp:\n",
    "    if (\".txt\" in each)==True: \n",
    "        files=files+[each]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions=[\"_PSD95_Pipe\",\"_MPP2_Pipe\",\"_SynCAM_Pipe\"]\n",
    "\n",
    "#create empty matrix\n",
    "paths=[path +\"/\"+ s for s in files]\n",
    "filematrix= [[]*5]*len(conditions)\n",
    "\n",
    "#fill matrix\n",
    "for i in range(0,len(conditions)):\n",
    "    tmp=conditions[i]\n",
    "    filematrix[i]= [x for x in paths if tmp in x]\n",
    "    \n",
    "filematrix=pd.DataFrame(filematrix).T\n",
    "filematrix.columns=conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the simulation from per file multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "nSimulations=10\n",
    "maxSubjects=10000#maximum number of subjects per simulation --> to minimize time. for best quality chose max\n",
    "#maxSubjects=2\n",
    "\n",
    "level=0.95#CI significance level\n",
    "pixelsize=0.04\n",
    "\n",
    "allNNs=pd.DataFrame()\n",
    "for i in range (nSimulations):\n",
    "\n",
    "    tmpNNs=pd.DataFrame()\n",
    "    #loop through all files\n",
    "    for k in trange (len(filematrix.index), desc=\"Images\",position=0):\n",
    "        try:\n",
    "            name=filematrix[\"_PSD95_Pipe\"][k]\n",
    "            print(name)\n",
    "            PSD=pd.read_table(name, sep=\"\\t\")\n",
    "            MPP2=pd.read_table(name.replace(\"_PSD95_Pipe\", \"_MPP2_Pipe\"), sep=\"\\t\")\n",
    "            SynCAM=pd.read_table(name.replace(\"_PSD95_Pipe\", \"_SynCAM_Pipe\"), sep=\"\\t\")\n",
    "\n",
    "            #create masks for objects within synapsesize and PSD in the center\n",
    "            psd=PSD[[\"Center of Mass (Ch 1): X (px)\",\"Center of Mass (Ch 1): Y (px)\"]].values*pixelsize\n",
    "            refMPP2=MPP2[[\"Center of Mass (Ch 2): X (px)\",\"Center of Mass (Ch 2): Y (px)\"]].values*pixelsize\n",
    "            refSynCAM=SynCAM[[\"Center of Mass (Ch 3): X (px)\",\"Center of Mass (Ch 3): Y (px)\"]].values*pixelsize\n",
    "            maskPSD_MPP2=np.min(scipy.spatial.distance_matrix(psd,refMPP2),axis=0)<diameterRegion/2 #mask for objects within synapse size\n",
    "            maskPSD_SynCAM=np.min(scipy.spatial.distance_matrix(psd,refSynCAM),axis=0)<diameterRegion/2 #mask for objects within synapse size\n",
    "\n",
    "            #select volumes within synapse\n",
    "            PSDvolsUM=PSD[\"Volume: Volume\"]*1000**6\n",
    "            SynCAMvolsUM=(SynCAM[\"Volume: Volume\"]*1000**6)[maskPSD_SynCAM]\n",
    "            MPP2volsUM=(MPP2[\"Volume: Volume\"]*1000**6  )[maskPSD_MPP2]\n",
    "\n",
    "\n",
    "            a=(NN(PSDvolsUM,SynCAMvolsUM,diameterRegion,(len(SynCAMvolsUM)/len(PSDvolsUM)),maxSubjects,False,True))\n",
    "            a=a.add_suffix(\"_Sub_PSD_Ref_SynCAM\")\n",
    "\n",
    "            b=(NN(PSDvolsUM,MPP2volsUM,diameterRegion,int(len(MPP2volsUM)/len(PSDvolsUM)),maxSubjects,False,False))\n",
    "            b=b.add_suffix(\"_Sub_PSD_Ref_MPP2\")\n",
    "\n",
    "            c=(NN(MPP2volsUM,SynCAMvolsUM,diameterRegion,int(len(SynCAMvolsUM)/len(PSDvolsUM)),maxSubjects,False,True))\n",
    "            c=c.add_suffix(\"_Sub_MPP2_Ref_SynCAM\")    \n",
    "\n",
    "            d=(NN(SynCAMvolsUM,PSDvolsUM,diameterRegion,1,maxSubjects,True,False))\n",
    "            d=d.add_suffix(\"_Sub_SynCAM_Ref_PSD\")\n",
    "\n",
    "            e=(NN(MPP2volsUM,PSDvolsUM,diameterRegion,1,maxSubjects,False,False))\n",
    "            e=e.add_suffix(\"_Sub_MPP2_Ref_PSD\")\n",
    "\n",
    "            f=(NN(SynCAMvolsUM,MPP2volsUM,diameterRegion,int(len(MPP2volsUM)/len(PSDvolsUM)),maxSubjects,True,False))   \n",
    "            f=f.add_suffix(\"_Sub_SynCAM_Ref_MPP2\")\n",
    "\n",
    "            g=(NN(SynCAMvolsUM,SynCAMvolsUM,diameterRegion,int(len(SynCAMvolsUM)/len(PSDvolsUM)),maxSubjects,True,True))   \n",
    "            g=g.add_suffix(\"_Sub_SynCAM_Ref_SynCAM\")\n",
    "\n",
    "            h=(NN(MPP2volsUM,MPP2volsUM,diameterRegion,int(len(MPP2volsUM)/len(PSDvolsUM)),maxSubjects,False,False))   \n",
    "            h=h.add_suffix(\"_Sub_MPP2_Ref_MPP2\")\n",
    "\n",
    "\n",
    "            NNs=  pd.concat([a,b,c,d,e,f,g,h], axis=1)\n",
    "\n",
    "            base=os.path.basename(filematrix[\"_PSD95_Pipe\"][k])\n",
    "            base[0:base.find(\".tif\")]\n",
    "            NNs=NNs.add_suffix((\"_\"+base))\n",
    "            tmpNNs=pd.concat([tmpNNs,NNs], axis=1)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Simulation \"+str(i)+\" of \"+str(nSimulations))\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    tmpNNs=tmpNNs.add_suffix((\"_Sim\"+str(i)))\n",
    "    allNNs=pd.concat([allNNs,tmpNNs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPath=path+\"/00simulations_raw\"\n",
    "if (os.path.exists(newPath)==False):\n",
    "    os.makedirs(newPath)\n",
    "\n",
    "allNNs.to_csv(newPath+\"/allSimulations_raw.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned=binData(allNNs,0,\"max\",0,1.6,20)\n",
    "newPath=path+\"/02_bin\"\n",
    "if (os.path.exists(newPath)==False):\n",
    "    os.makedirs(newPath)\n",
    "binned.to_csv(path+\"/02_bin/02bin.txt\")\n",
    "    \n",
    "norm_bin=normalize(binned)\n",
    "newPath=path+\"/03norm_bin\"\n",
    "if (os.path.exists(newPath)==False):\n",
    "    os.makedirs(newPath)\n",
    "norm_bin.index=binned.index.values\n",
    "norm_bin.to_csv(path+\"/03norm_bin/03norm_bin.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do statistics and calculate the CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate number of simulation rounds\n",
    "simulations=[]\n",
    "for i in allNNs.columns:\n",
    "    simulations = simulations + [i[i.find(\"Sim\"):]]\n",
    "simulations=list(set(simulations))\n",
    "\n",
    "\n",
    "#define keywords to search for and assign/group conditions\n",
    "identA=[\"NNcenter_Sub_PSD_Ref_SynCAM\",\n",
    "        \"NNcenter_Sub_PSD_Ref_MPP2\",\n",
    "        \"NNcenter_Sub_MPP2_Ref_SynCAM\",\n",
    "        \"NNcenter_Sub_SynCAM_Ref_PSD\",\n",
    "        \"NNcenter_Sub_MPP2_Ref_PSD\",\n",
    "        \"NNcenter_Sub_SynCAM_Ref_MPP2\",\n",
    "        \"NNcenter_Sub_SynCAM_Ref_SynCAM\",\n",
    "        \"NNcenter_Sub_MPP2_Ref_MPP2\",\n",
    "        \n",
    "        \"NNsurface_Sub_PSD_Ref_SynCAM\",\n",
    "        \"NNsurface_Sub_PSD_Ref_MPP2\",\n",
    "        \"NNsurface_Sub_MPP2_Ref_SynCAM\",\n",
    "        \"NNsurface_Sub_SynCAM_Ref_PSD\",\n",
    "        \"NNsurface_Sub_MPP2_Ref_PSD\",\n",
    "        \"NNsurface_Sub_SynCAM_Ref_MPP2\",\n",
    "        \"NNsurface_Sub_SynCAM_Ref_SynCAM\",\n",
    "        \"NNsurface_Sub_MPP2_Ref_MPP2\",]\n",
    "        \n",
    "#conditions to average\n",
    "identB=[\"No23\",\"No_3\",\"_01-DIV28_\"]#categories to average\n",
    "\n",
    "\n",
    "# do statistics on groups\n",
    "for i in range (0, len(identA)):     \n",
    "    for h in range(0,len(simulations)):         \n",
    "        for j in range (0, len(identB)):\n",
    "            \n",
    "            #split conditions\n",
    "            split_norm_bin=split(split(split(norm_bin,simulations[h],1),identA[i],1),identB[j],1)\n",
    "            split_norm_bin.index=binned.index.values\n",
    "            newPath=path+\"/04_split_norm_bin\"\n",
    "            if (os.path.exists(newPath)==False):\n",
    "                os.makedirs(newPath)\n",
    "            split_norm_bin.to_csv(newPath+\"/04\"+identA[i]+identB[j]+simulations[h]+\"_norm_bin.txt\")\n",
    "                        \n",
    "            if (j==0):\n",
    "                summary = np.array(split_norm_bin.mean(axis=1))\n",
    "                header = [identA[i]+identB[j]+simulations[h]]\n",
    "            else:\n",
    "                summary = np.vstack((summary , split_norm_bin.mean(axis=1)))\n",
    "                header = header+[identA[i]+identB[j]+simulations[h]]\n",
    "                \n",
    "        #statistics on averages        \n",
    "        stats=np.vstack((\n",
    "                        pd.DataFrame(summary).T.mean(axis=1),\n",
    "                        pd.DataFrame(summary).T.median(axis=1),\n",
    "                        pd.DataFrame(summary).T.std(axis=1),\n",
    "                        pd.DataFrame(summary).T.sem(axis=1),\n",
    "                        summary))\n",
    "        header=[\"mean\"]+[\"median\"]+[\"std\"]+[\"sem\"]+header\n",
    "        \n",
    "        stats=pd.DataFrame(stats.T, columns=header)\n",
    "        stats.index=binned.index.values\n",
    "        newPath=path+\"/05_stats_split_norm_bin\"\n",
    "        if (os.path.exists(newPath)==False):\n",
    "                os.makedirs(newPath)\n",
    "        stats.to_csv(newPath+\"/05\"+identA[i]+\"statistics_average_norm_bin_\"+simulations[h]+\".txt\")\n",
    "        \n",
    "        \n",
    "        #statistics on normalized average\n",
    "\n",
    "        summary=normalize(summary)\n",
    "        nstats=np.vstack((\n",
    "                        pd.DataFrame(summary).T.mean(axis=1),\n",
    "                        pd.DataFrame(summary).T.median(axis=1),\n",
    "                        pd.DataFrame(summary).T.std(axis=1),\n",
    "                        pd.DataFrame(summary).T.sem(axis=1),\n",
    "                        summary))\n",
    "         \n",
    "        \n",
    "        nstats=pd.DataFrame(nstats.T, columns=header)\n",
    "        nstats.index=binned.index.values\n",
    "        newPath=path+\"/06_stats_on_stats\"\n",
    "        if (os.path.exists(newPath)==False):\n",
    "                os.makedirs(newPath)\n",
    "        nstats.to_csv(newPath+\"/06\"+\"statistics_norm_average_norm_bin_\"+simulations[h]+identA[i]+\".txt\") \n",
    "       \n",
    "    \n",
    "    #calculate stastistics over simulations\n",
    "\n",
    "        if (h==0):\n",
    "            simStats=pd.DataFrame(nstats[\"mean\"])\n",
    "        else:\n",
    "            simStats=pd.concat([simStats,nstats[\"mean\"]],axis=1)\n",
    "    \n",
    "    simStats.columns=simulations\n",
    "    simStats=simStats.add_suffix(\"_\"+identA[i])\n",
    "    \n",
    "    CIsUP=simStats.quantile(q=(1-(1-level)/2), axis=1,interpolation='midpoint')\n",
    "    CIsLOW=simStats.quantile(q=((1-level)/2), axis=1,interpolation='midpoint')\n",
    "    \n",
    "    simStats=pd.concat([\n",
    "                        CIsUP,\n",
    "                        CIsLOW,\n",
    "                        simStats.T.mean(),\n",
    "                        simStats.T.median(),\n",
    "                        simStats.T.std(),\n",
    "                        simStats.T.sem(),\n",
    "                        simStats\n",
    "                        ],axis=1)\n",
    "    simStats.columns=[\"upper_CI\"]+[\"lower_CI\"]+[\"mean\"]+[\"median\"]+[\"std\"]+[\"sem\"]+simulations\n",
    "    newPath=path+\"/07_CI_from_\"+str(len(simulations))+\"simulations\"\n",
    "    if (os.path.exists(newPath)==False):\n",
    "            os.makedirs(newPath)\n",
    "    simStats.to_csv(newPath+\"/07_CI_from_\"+str(len(simulations))+\"simulations\"+identA[i]+\".txt\") \n",
    "    \n",
    "\n",
    "    fig=simStats[[\"upper_CI\",\"lower_CI\",\"mean\"]].plot()\n",
    "    #fig.suptitle(identA[i], fontsize=14, fontweight='bold')\n",
    "    fig.set_title(identA[i], fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"NN distance (µm)\")\n",
    "    plt.ylabel(\"Normalized Frequency\")                       \n",
    "    plt.axis([0, 2, 0, nstats[\"mean\"].max()])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=1, borderaxespad=0.)\n",
    "    plt.savefig(path+\"/08_\"+identA[i]+\"simulated_kNN.png\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the CIs from the simulation against the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identA_data=[\n",
    "\"Inverse_C_Sch3-Rch1\",    \n",
    "\"Inverse_C_Sch2-Rch1\",\n",
    "\"Inverse_C_Sch3-Rch2\",    \n",
    "\"x_C_Sch3-Rch1\",   \n",
    "\"x_C_Sch2-Rch1\",    \n",
    "\"x_C_Sch3-Rch2\",\n",
    "\"x_C_Sch3-Rch3\",\n",
    "\"x_C_Sch2-Rch2\",\n",
    "\"Inverse_S_Sch3-Rch1\",    \n",
    "\"Inverse_S_Sch2-Rch1\",\n",
    "\"Inverse_S_Sch3-Rch2\",    \n",
    "\"x_S_Sch3-Rch1\",   \n",
    "\"x_S_Sch2-Rch1\",    \n",
    "\"x_S_Sch3-Rch2\",\n",
    "\"x_S_Sch3-Rch3\",\n",
    "\"x_S_Sch2-Rch2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pair the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulationfolder=r\"....\\_Annotation_And_Simulations\\noShift_and_simulations\\07_CI_from_10simulations\"+\"\\\\\"\n",
    "oneSimulationfolder=r\"....\\_Annotation_And_Simulations\\noShift_and_simulations\\06_stats_on_stats\"+\"\\\\\"\n",
    "datafolder=r\"....\\Session1-3\\_NN\\NEWNEW\\txt\\06_stats_on_stats\"+\"\\\\\"\n",
    "\n",
    "simulationfiles=[simulationfolder + s for s in os.listdir(simulationfolder)]\n",
    "oneSimulationfiles=[oneSimulationfolder + s for s in os.listdir(oneSimulationfolder)]\n",
    "datafiles=[datafolder + s for s in os.listdir(datafolder)]\n",
    "\n",
    "\n",
    "data=[]\n",
    "simulations=[]\n",
    "oneSimulations=[]\n",
    "\n",
    "\n",
    "#fill matrix\n",
    "for i in range(0,len(identA_data)):\n",
    "    tmp=identA_data[i]\n",
    "    #data[i]= [x for x in datafiles if tmp in x]\n",
    "    data= data+[x for x in datafiles if tmp in x and \"Noshift\" in x]\n",
    "    \n",
    "for i in range(0,len(identA)):\n",
    "    tmp=identA[i]\n",
    "    simulations=simulations+ [x for x in simulationfiles if tmp in x]\n",
    "    oneSimulations=oneSimulations+ [x for x in oneSimulationfiles if tmp in x and \"Sim0\" in x]\n",
    "    \n",
    "filematrix=pd.DataFrame([data,simulations,oneSimulations])\n",
    "filematrix.columns=identA\n",
    "filematrix.index=[\"data\",\"simulations\",\"oneSimulations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range (0, len(identA)):\n",
    "\n",
    "    thisData=pd.read_csv(filematrix[identA[i]][\"data\"], sep=\",\")\n",
    "    thisSimulation=pd.read_csv(filematrix[identA[i]][\"simulations\"], sep=\",\")\n",
    "    thisPlot=pd.concat([thisData[\"mean\"],thisSimulation[[\"upper_CI\",\"lower_CI\"]]], axis=1)\n",
    "    thisPlot.index=thisData.index=thisSimulation.index=thisData['Unnamed: 0']\n",
    "\n",
    "    dummy = pd.DataFrame(index=thisData.index, columns=[\"dummy\"])\n",
    "    dummy.fillna(\"NaN\") \n",
    "    fig=thisPlot[[\"upper_CI\",\"lower_CI\",\"mean\"]].plot(yerr = [dummy[\"dummy\"],dummy[\"dummy\"],thisData[\"sem\"]])\n",
    "    #fig=thisPlot[[\"upper_CI\",\"lower_CI\",\"mean\"]].plot()\n",
    "    \n",
    "    fig.set_title(identA[i]+\"_\"+identA_data[i], fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"NN distance (µm)\")\n",
    "    plt.ylabel(\"Normalized Frequency\")                       \n",
    "    plt.axis([0, 1.6, 0, max(thisPlot.max())*1.2])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=1, borderaxespad=0.)\n",
    "    plt.savefig(path+\"/09_\"+identA[i]+\"simulated_vs_raw_kNN.png\", dpi=1000)\n",
    "    #thisPlot.to_excel(path+\"/09_\"+identA[i]+\"simulated_vs_raw_kNN.xls\")\n",
    "    \n",
    "    export=pd.concat([thisData[\"mean\"],thisData[\"sem\"],thisSimulation[[\"upper_CI\",\"lower_CI\"]]], axis=1)\n",
    "    export.columns=[\"mean data\",\"sem data\",\"upper_CI\",\"lower_CI\"]\n",
    "    export.to_excel(path+\"/09_\"+identA[i]+\"simulated_vs_raw_kNN.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot only statistics on Simulation 0  - this can be combines with bootstrap method for the supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range (0, len(identA)):\n",
    "\n",
    "    thisData=pd.read_csv(filematrix[identA[i]][\"data\"], sep=\",\")\n",
    "    thisSimulation=pd.read_csv(filematrix[identA[i]][\"oneSimulations\"], sep=\",\")\n",
    "    thisPlot=pd.concat([thisData[\"mean\"],thisSimulation[\"mean\"]], axis=1)\n",
    "    thisPlot.columns=[\"data_mean\",\"simulation_mean\"]\n",
    "    thisPlot.index=thisData.index=thisSimulation.index=thisData['Unnamed: 0']\n",
    "\n",
    "    fig=thisPlot[[\"data_mean\",\"simulation_mean\"]].plot(yerr = [thisData[\"sem\"],thisSimulation[\"sem\"]])    \n",
    "    fig.set_title(identA[i]+\"_\"+identA_data[i], fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"NN distance (µm)\")\n",
    "    plt.ylabel(\"Normalized Frequency\")                       \n",
    "    plt.axis([0, 1.6, 0, max(thisPlot.max())*1.2])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=1, borderaxespad=0.)\n",
    "    plt.savefig(path+\"/10\"+identA[i]+\"simulated_sem_vs_raw_kNN.png\", dpi=1000)\n",
    "    export=pd.concat([thisData[\"mean\"],thisData[\"sem\"],thisSimulation[\"mean\"],thisSimulation[\"sem\"]], axis=1)\n",
    "    export.columns=[\"mean data\",\"sem data\",\"mean simulation\",\"sem simulation\"]\n",
    "    export.to_excel(path+\"/10\"+identA[i]+\"simulated_sem_vs_raw_kNN.xls\")\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig=plt.figure()\n",
    "    x=thisPlot.index.values\n",
    "    y=np.divide(np.subtract(thisPlot[\"data_mean\"].values,thisPlot[\"simulation_mean\"].values),thisPlot[\"simulation_mean\"].values)\n",
    "    plt.plot(x[0:10],y[0:10])\n",
    "    plt.title(identA[i]+\"_\"+identA_data[i], fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"NN distance (µm)\")\n",
    "    plt.ylabel(\"(Normalized Frequency -Random) / Random\")                       \n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=1, borderaxespad=0.)\n",
    "    plt.savefig(path+\"/11\"+identA[i]+\"simulated_sem_vs_accumulation_kNN.png\", dpi=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
